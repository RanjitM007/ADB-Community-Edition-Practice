{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b63944f7-f00d-421e-bdcf-9ed0d97e71cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ List of Operations Performed\n",
    "\n",
    "1. Load data from the source table `workspace.de_practice_source.sales` using Spark. — [Supporting Document (Azure - `read.table`)](https://learn.microsoft.com/en-us/azure/databricks/data/tables#read-tables)\n",
    "2. Display the loaded DataFrame. — \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f23076-af77-4abf-a242-874b11cbd97b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Source Data"
    }
   },
   "outputs": [],
   "source": [
    "#Load Data From Source\n",
    "source = spark.read.table('workspace.de_practice_source.sales')\n",
    "source.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce914109-0784-4e7a-965e-3ce022eb1037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7ced82-92cf-4452-be84-773d758dd4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Primary Key: fransiseid\n",
    "- Hash Key: [create hash key]\n",
    "- Created Date: [add created date]\n",
    "- Modified Date: [add modified date]\n",
    "- indcurrent record: 0 (not active), 1 (active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7844d0-0e37-47a8-80d9-78f2803aae06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ List of Operations Performed\n",
    "\n",
    "1. Load data from the source DataFrame.\n",
    "2. Concatenate all columns into a single column named `ConCatValue` using no delimiter. — [Supporting Document (Azure - `concat_ws` Function)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/concat_ws)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54815cc-0a40-44a6-964f-7fbe9fbba515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load Data From Source and concatenate all columns into 'ConCatValue'\n",
    "source = source.withColumn('ConCatValue', F.concat_ws('', *source.columns))\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a5e0c13-30ae-4d54-95d2-9558971ecc84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ List of Operations Performed\n",
    "\n",
    "1. Add a new column `IndCurrent` with a constant value of `1`. — [Supporting Document ( `lit` Function)](https://api-docs.databricks.com/python/pyspark/latest/pyspark.sql/api/pyspark.sql.functions.lit.html)\n",
    "2. Add a new column `CreatedDate` with the current timestamp. — [Supporting Document (Azure - `current_timestamp`)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/current_timestamp)\n",
    "3. Add a new column `ModifiedDate` with the current timestamp. — [Supporting Document (Azure - `current_timestamp`)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/current_timestamp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11c239ba-389d-41d1-8ed5-5c1c74278183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add IndCurrent, CreatedDate, and ModifiedDate columns\n",
    "source = source.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e729bc-2d93-43a1-b2bf-40dc98191a31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.orderBy(F.monotonically_increasing_id())\n",
    "source = source.withColumn(\"storage_id\", F.row_number().over(window_spec))\n",
    "\n",
    "first_cols = [\"storage_id\"]\n",
    "other_cols = [col for col in source.columns if col not in first_cols]\n",
    "source = source.select(first_cols + other_cols)\n",
    "\n",
    "display(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0be2b5af-dbeb-4bd9-be3c-ac573f3ffa7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ List of Operations Performed\n",
    "\n",
    "1. Generate SHA-256 hash from the `ConCatValue` column. — [Supporting Document (Azure)](https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/sha2)\n",
    "2. Create a new column named `RowHash` to store the hash. — [Supporting Document (Azure)](https://learn.microsoft.com/en-us/azure/databricks/pyspark/basics#create-columns)\n",
    "3. Drop the intermediate column `ConCatValue`. — [Supporting Document (Azure)](https://learn.microsoft.com/en-us/azure/databricks/pyspark/basics#remove-columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d37580d-d3dd-4ece-891c-e3f481ed3ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate SHA-256 hash of concatenated column values and drop 'ConCatValue'\n",
    "source = source.withColumn(\"RowHash\", F.sha2(F.col(\"ConCatValue\"), 256)).drop('ConCatValue')\n",
    "display(source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ec1deb-45eb-4ec4-a046-4299aa5c6755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✅ List of Operations Performed\n",
    "\n",
    "1. Write data from the source DataFrame to the target table `workspace.de_practice_target.target` using append mode. — [Supporting Document (Azure - `saveAsTable`)](https://learn.microsoft.com/en-us/azure/databricks/data/tables#write-to-tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5db10bf-4945-4ad8-8dd9-10eb0eefe04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#writing to the target schema  \n",
    "source.write.mode(\"append\").saveAsTable(\"workspace.de_practice_target.sales\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Load From Source To Target initial",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
