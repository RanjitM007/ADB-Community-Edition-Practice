{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c61e8dc-61c4-452d-a319-f8c181220082",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Variable Declarartion"
    }
   },
   "outputs": [],
   "source": [
    "SourceTable='workspace.de_practice_source.sales'\n",
    "TargetTable='workspace.de_practice_target.sales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dfe1548-fd6f-4af6-9395-8f7d22ef8e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf=spark.read.table(SourceTable)  # Read source table into DataFrame\n",
    "TargetDf=spark.read.table(TargetTable)  # Read target table into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b52a488-c51f-4eb6-8386-c18b944340a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Filter the DataFrame to show only rows where 'franchiseID' is '3000001'\n",
    "# Display the filtered DataFrame for inspection\n",
    "SourceDf.filter(col(\"franchiseID\") == \"3000001\").display()\n",
    "\n",
    "# The 'city' value for rows with 'franchiseID' 3000001 is 'Tokyo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1afd5743-1589-4863-932d-9fd3864ff7d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Update the 'city' column in SourceDf:\n",
    "# For rows where 'franchiseID' equals '3000001', set the 'city' value to 'Tokyo Modified'.\n",
    "# For all other rows, retain the original 'city' value.\n",
    "SourceDf = SourceDf.withColumn(\n",
    "    \"city\",\n",
    "    when(col(\"franchiseID\") == \"3000001\", \"Mumbai\").otherwise(col(\"city\"))\n",
    ")\n",
    "\n",
    "# Display rows where 'franchiseID' is '3000001' to verify the 'city' column update.\n",
    "SourceDf.filter(col(\"franchiseID\") == \"3000001\").display()\n",
    "\n",
    "# After this update, the 'city' value for all rows with 'franchiseID' 3000001 will be 'Delhi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc887422-4d0a-4fb2-ac95-65862a24aa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d463ad8d-6eb2-43c4-bb10-fb8025dc2f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a hash key by concatenating all columns into a single string column 'RowHash'\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Concatenate all columns in 'source' DataFrame into 'RowHash'\n",
    "SourceDf = SourceDf.withColumn('RowHash', F.sha2(F.concat_ws('', *SourceDf.columns), 256))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ff20a6-e407-4c9b-afe6-76372a17ad57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add three new columns to SourceDf:\n",
    "# 1. 'IndCurrent': Set to 1 for all rows, indicating the current/active record.\n",
    "# 2. 'CreatedDate': Set to the current timestamp, representing when the record was created.\n",
    "# 3. 'ModifiedDate': Set to the current timestamp, representing when the record was last modified.\n",
    "SourceDf = SourceDf.withColumn(\"IndCurrent\", F.lit(1)) \\\n",
    "    .withColumn(\"CreatedDate\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ModifiedDate\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac8d2e1-fcf0-4745-831e-0039c7a52b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SourceDf.filter(col(\"franchiseID\") == \"3000001\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5800a81-9398-43a4-8806-688e6c3c4df1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1751717599322}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#join with Target Table and create Flag\n",
    "TargetDf=spark.read.table(TargetTable).select(['franchiseID','RowHash','storage_id']).withColumnRenamed('RowHash','TargetHash')\n",
    "SourceDf=SourceDf.join(TargetDf, on =['franchiseID'], how='left').withColumn('Flag', F.when(col('TargetHash').isNull() | (col('TargetHash') != col('RowHash')), 'New').when(col('TargetHash') == col('RowHash'), 'NoChange').otherwise('Update'))\n",
    "# Drop the TargetHash column\n",
    "SourceDf=SourceDf.drop('TargetHash')\n",
    "SourceDf=SourceDf.filter(col(\"Flag\") == \"New\")\n",
    "SourceDf.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8ac639-8799-4ca6-96e0-56c2b8dbd9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ðŸ“˜ What is SCD Type 2?\n",
    "\n",
    "**SCD (Slowly Changing Dimension) Type 2** is a technique used in data warehousing to handle changes in dimension data **while preserving historical records**.\n",
    "\n",
    "Whenever an attribute of a dimension changes (e.g., a customer's city), instead of overwriting the existing record, **SCD Type 2 inserts a new row** with the updated value. The old row is retained to maintain the historical context of data.\n",
    "\n",
    "This method ensures that historical analysis remains accurate based on what was true **at the time** of the event or transaction.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Scenario Example (General):\n",
    "\n",
    "Letâ€™s say you store customer information, and the customer's city changes from **Seattle** to **Redmond**.  \n",
    "With **SCD Type 2**, instead of overwriting the existing record, you **insert a new row** with the updated city while **retaining the old row**.\n",
    "\n",
    "This approach helps preserve historical accuracy, allowing analysis based on what was true **at the time** the data was recorded.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Scenario Example (Tabular):\n",
    "\n",
    "- **Primary Key:** `frinsied_id`  \n",
    "- **Changed Attribute:** `city`  \n",
    "- **Flag Column:** `indcurrent` (1 = active, 0 = inactive)\n",
    "\n",
    "Suppose a record with `frinsied_id = 3000001` originally had `city = 'Tokyo'`. Later, the city is updated to `'Delhi'`.\n",
    "\n",
    "With **SCD Type 2**, the table will look like this:\n",
    "\n",
    "| surrogate_key | frinsied_id | city   | start_date | end_date   | indcurrent |\n",
    "|---------------|-------------|--------|------------|------------|------------|\n",
    "| 1             | 3000001     | Tokyo  | 2022-01-01 | 2023-05-15 | 0          |\n",
    "| 2             | 3000001     | Delhi  | 2023-05-16 | null       | 1          |\n",
    "\n",
    "> The original value `'Tokyo'` is preserved as a historical record.  \n",
    "> The new record `'Delhi'` is added with `indcurrent = 1`, and the previous one is marked as inactive (`indcurrent = 0`).\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ“– [Learn more: Slowly Changing Data â€“ Microsoft Azure Data Factory](https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview#slowly-changing-data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094bba44-ae7f-4bc9-a2db-b2e32e21d117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Before applying the SCD Type 1 merge, let's inspect the data in the target table for a specific franchiseID\n",
    "display(spark.sql(\"select * from workspace.de_practice_target.sales where franchiseID='3000001'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0b0482-5621-4329-a4e8-058514206e39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp, lit, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "\n",
    "# Configuration\n",
    "table_name = \"workspace.de_practice_target.sales\"\n",
    "key_column = \"franchiseID\"\n",
    "hash_column = \"RowHash\"\n",
    "is_current_column = \"IndCurrent\"\n",
    "surrogate_key_column = \"storage_id\"\n",
    "created_column = \"CreatedDate\"\n",
    "\n",
    "# Reference Delta table\n",
    "target_table = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Add new columns to source DataFrame\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "SourceDf = SourceDf \\\n",
    "    .withColumn(surrogate_key_column, uuid_udf()) \\\n",
    "    .withColumn(created_column, current_timestamp()) \\\n",
    "    .withColumn(is_current_column, lit(1))\n",
    "\n",
    "# Use aliases properly\n",
    "src = SourceDf.alias(\"src\")\n",
    "tgt = target_table.alias(\"tgt\")\n",
    "\n",
    "# Use column expressions (not strings) in merge condition\n",
    "tgt.merge(\n",
    "    source=src,\n",
    "    condition=(\n",
    "        (col(f\"tgt.{key_column}\") == col(f\"src.{key_column}\")) &\n",
    "        (col(f\"tgt.{is_current_column}\") == lit(1))\n",
    "    )\n",
    ").whenMatchedUpdate(\n",
    "    condition=col(f\"tgt.{hash_column}\") != col(f\"src.{hash_column}\"),\n",
    "    set={\n",
    "        is_current_column: lit(0)\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03e53ebb-1394-4bf3-9ec4-15b6ad12d442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the target table data for a specific franchiseID after applying the SCD Type 2 merge\n",
    "display(spark.sql(\"select * from workspace.de_practice_target.sales where franchiseID='3000001'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40af35d9-215a-412b-9d16-396117fb4dd3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update the Storage-id for new records"
    }
   },
   "outputs": [],
   "source": [
    "SourceDf = SourceDf.drop('storage_id','Flag')\n",
    "max_storage_id = spark.sql(f\"select max(storage_id) as max_id from {table_name}\").first()['max_id']\n",
    "next_storage_id = 1 if not max_storage_id or max_storage_id == 0 else max_storage_id + 1\n",
    "\n",
    "SourceDf = SourceDf.withColumn('storage_id', lit(next_storage_id))\n",
    "SourceDf = SourceDf.withColumn('IndCurrent', lit(1))\n",
    "SourceDf.write.format('delta').mode('append').saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cbc4d17-ae2f-4fca-8d9e-61a535d620d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the target table data for a specific franchiseID after applying the SCD Type 2 merge\n",
    "display(spark.sql(\"select * from workspace.de_practice_target.sales where franchiseID='3000001'\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_Type_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
